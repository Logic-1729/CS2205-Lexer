#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–æµ‹è¯•è¯æ³•åˆ†æå™¨ ./regex_automata 1
æµ‹è¯•ç”¨ä¾‹ä½äº ./lexer_cases/ ç›®å½•ä¸‹çš„ .txt æ–‡ä»¶ä¸­
"""
import subprocess
import sys
import re
import os
from pathlib import Path

SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent
LEXER_EXE = PROJECT_ROOT / "regex_automata"

if not os.path.isfile(LEXER_EXE):
    print(f"âŒ æ‰¾ä¸åˆ°è¯æ³•åˆ†æå™¨: {os.path.abspath(LEXER_EXE)}")
    print("è¯·ç¡®ä¿åœ¨ CS2205-Lexer/ ç›®å½•ä¸‹æœ‰å¯æ‰§è¡Œæ–‡ä»¶ 'regex_automata'")
    sys.exit(1)


def parse_expected_tokens(lines):
    """ä»é¢„æœŸè¡Œä¸­æå– (token_type, lexeme) åˆ—è¡¨"""
    tokens = []
    for line in lines:
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        match = re.match(r'^([A-Z_][A-Z0-9_]*)\s+"(.*)"$', line)
        if match:
            tokens.append((match.group(1), match.group(2)))
        else:
            print(f"âš ï¸ è­¦å‘Šï¼šæ— æ³•è§£æé¢„æœŸè¡Œ: {line}")
    return tokens


def run_lexer_on_input(input_str):
    """è¿è¡Œ ./regex_automata 1 å¹¶è¿”å›å®é™… token åˆ—è¡¨"""
    try:
        proc = subprocess.Popen(
            [LEXER_EXE, "1"],
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            encoding="utf-8",
            cwd=PROJECT_ROOT,  # ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
        )
        stdout, stderr = proc.communicate(input=input_str + "\nquit\n", timeout=10)

        if proc.returncode != 0:
            print(f"âŒ è¯æ³•åˆ†æå™¨å´©æºƒï¼Œstderr:\n{stderr}")
            return None

        lines = stdout.splitlines()
        in_table = False
        actual_tokens = []
        for line in lines:
            if "Tokens:" in line:
                in_table = True
                continue
            if in_table and "Total:" in line:
                break
            if (
                in_table
                and "â”‚" in line
                and "Token Type" not in line
                and "Line" not in line
            ):
                parts = [p.strip() for p in line.split("â”‚")[1:-1]]
                if len(parts) >= 4:
                    token_type = parts[2]
                    lexeme = parts[3]
                    if lexeme.startswith('"') and lexeme.endswith('"'):
                        lexeme = lexeme[1:-1]
                    actual_tokens.append((token_type, lexeme))
        return actual_tokens

    except subprocess.TimeoutExpired:
        proc.kill()
        print("âŒ è¯æ³•åˆ†æå™¨è¶…æ—¶ï¼ˆå¯èƒ½æ­»å¾ªç¯ï¼‰")
        return None
    except Exception as e:
        print(f"âŒ è¿è¡Œè¯æ³•åˆ†æå™¨å‡ºé”™: {e}")
        return None


def load_test_cases_from_file(file_path):
    """ä»å•ä¸ªæ–‡ä»¶åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
    test_cases = []
    current_input = None
    current_expected = []

    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.rstrip("\n")
            if line.startswith(">>> "):
                if current_input is not None:
                    expected = parse_expected_tokens(current_expected)
                    test_cases.append((current_input, expected, str(file_path)))
                current_input = line[4:]
                current_expected = []
            else:
                current_expected.append(line)

        if current_input is not None:
            expected = parse_expected_tokens(current_expected)
            test_cases.append((current_input, expected, str(file_path)))

    return test_cases


def main():
    lexer_cases_dir = SCRIPT_DIR / "lexer_cases"
    if not lexer_cases_dir.exists():
        print(f"âŒ æµ‹è¯•ç”¨ä¾‹ç›®å½•ä¸å­˜åœ¨: {lexer_cases_dir}")
        sys.exit(1)

    # æ”¶é›†æ‰€æœ‰ .txt æµ‹è¯•æ–‡ä»¶
    test_files = list(lexer_cases_dir.glob("*.txt"))
    if not test_files:
        print(f"âŒ æœªæ‰¾åˆ°ä»»ä½•æµ‹è¯•æ–‡ä»¶ (*.txt) åœ¨ {lexer_cases_dir}")
        sys.exit(1)

    all_test_cases = []
    for test_file in sorted(test_files):
        cases = load_test_cases_from_file(test_file)
        all_test_cases.extend(cases)

    print(f"ğŸ§ª å…±åŠ è½½ {len(all_test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    print("-" * 50)

    passed = 0
    failed = 0

    for i, (input_str, expected, source_file) in enumerate(all_test_cases, 1):
        print(
            f"\n[{i}/{len(all_test_cases)}] æµ‹è¯•: {repr(input_str)} (æ¥è‡ª {os.path.basename(source_file)})"
        )
        actual = run_lexer_on_input(input_str)

        if actual is None:
            print("âŒ æµ‹è¯•å¤±è´¥ï¼šè¯æ³•åˆ†æå™¨æœªæ­£å¸¸è¿”å›")
            failed += 1
            continue

        if actual == expected:
            print("âœ… é€šè¿‡")
            passed += 1
        else:
            print("âŒ å¤±è´¥")
            print(f"  æœŸæœ›: {expected}")
            print(f"  å®é™…: {actual}")
            failed += 1

    print("\n" + "=" * 60)
    print(f"âœ… æ€»ç»“: {passed} é€šè¿‡, {failed} å¤±è´¥")
    print("=" * 60)

    if failed > 0:
        sys.exit(1)
    else:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")


if __name__ == "__main__":
    main()
